{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neuralnet.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPVwp4H5PNa5SLSwdgrgDm7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/littlejacinthe/torchaudio/blob/main/Neuralnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Tutorial by Valerio Velardo\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "3vYsw87ma5p8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor"
      ],
      "metadata": {
        "id": "qJM2fCL7a_LM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 128\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 0.001"
      ],
      "metadata": {
        "id": "TB6WWNSrlZ6L"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardNet(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__() # nn.Module functions\n",
        "\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.dense_layers = nn.Sequential( #trick to get several layers into one component\n",
        "        nn.Linear(28*28, 256), #images in the dataset are of size 28x28 --> Flattened\n",
        "        nn.ReLU(), #activation layer\n",
        "        nn.Linear(256, 10) #output layer\n",
        "    )\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "  def forward(self, input_data): #how to manipulate the data\n",
        "    flattened_data = self.flatten(input_data)\n",
        "    logits = self.dense_layers(flattened_data)\n",
        "    predictions = self.softmax(logits)\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "I0cFhWjpk2gv"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_mnist_datasets():\n",
        "\n",
        "  train_data = datasets.MNIST(\n",
        "      root=\"data\",\n",
        "      download=True,\n",
        "      train=True,\n",
        "      transform=ToTensor() # normalized btw 0 and 1\n",
        "  )\n",
        "\n",
        "  validation_data = datasets.MNIST(\n",
        "      root=\"data\",\n",
        "      download=True,\n",
        "      train=False,\n",
        "      transform=ToTensor() # normalized btw 0 and 1\n",
        "  )\n",
        "  \n",
        "  return train_data, validation_data"
      ],
      "metadata": {
        "id": "1dh6K-ukcrb3"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, data_loader, loss_fn, optimiser, device):\n",
        "  \n",
        "  for inputs, targets in data_loader:\n",
        "    inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "    #calculate loss\n",
        "    predictions = model(inputs)\n",
        "    loss = loss_fn(predictions, targets)\n",
        "\n",
        "    #backpropagate loss and update weights\n",
        "    optimiser.zero_grad() # the optimiser calculates gradients at each iteration to decide how to update the weights, this resets the gradients to 0\n",
        "    loss.backward()\n",
        "    optimiser.step()\n",
        "\n",
        "\n",
        "  print(f\"Loss: {loss.item()}\") #prints the loss for the last batch \n",
        "\n",
        "\n",
        "def train(model, data_loader, loss_fn, optimiser, device, epochs):\n",
        "  \n",
        "  for i in range(epochs):\n",
        "    print(f\"Epoch {i+1}\")\n",
        "    train_one_epoch(model, data_loader, loss_fn, optimiser, device)\n",
        "    print(\"------------------------\")\n",
        "\n",
        "  print(\"Training done\")"
      ],
      "metadata": {
        "id": "btCiivEkokBL"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.modules.loss import CrossEntropyLoss\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "  # download dataset\n",
        "  train_data, _ = download_mnist_datasets()\n",
        "  print(\"MNIST dataset downloaded\")\n",
        "\n",
        "  # create dataloader for dataset\n",
        "  train_data_loader = DataLoader(train_data, batch_size=BATCH_SIZE)\n",
        "\n",
        "  #build model\n",
        "  if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "  else:\n",
        "    device = \"cpu\"\n",
        "  print(f\"Using {device} device\")\n",
        "\n",
        "  feed_forward_net = FeedForwardNet().to(device)\n",
        "\n",
        "  #instantiate loss function & optimiser\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  optimiser = torch.optim.Adam(feed_forward_net.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "  #train model\n",
        "  train(feed_forward_net, train_data_loader, loss_fn, optimiser, device, EPOCHS)\n",
        "\n",
        "  #store the model\n",
        "  torch.save(feed_forward_net.state_dict(), \"feedforwardnet.pth\")\n",
        "  print(\"Model trained and stored at feedforwardnet.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSBrQx6KkVDT",
        "outputId": "d66f6334-9e80-4120-bdbb-a27df8e7a21d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MNIST dataset downloaded\n",
            "Using cpu device\n",
            "Epoch 1\n",
            "Loss: 1.5094488859176636\n",
            "------------------------\n",
            "Epoch 2\n",
            "Loss: 1.4943381547927856\n",
            "------------------------\n",
            "Epoch 3\n",
            "Loss: 1.4868143796920776\n",
            "------------------------\n",
            "Epoch 4\n",
            "Loss: 1.4790884256362915\n",
            "------------------------\n",
            "Epoch 5\n",
            "Loss: 1.4740031957626343\n",
            "------------------------\n",
            "Epoch 6\n",
            "Loss: 1.4727061986923218\n",
            "------------------------\n",
            "Epoch 7\n",
            "Loss: 1.4728583097457886\n",
            "------------------------\n",
            "Epoch 8\n",
            "Loss: 1.4728723764419556\n",
            "------------------------\n",
            "Epoch 9\n",
            "Loss: 1.4723901748657227\n",
            "------------------------\n",
            "Epoch 10\n",
            "Loss: 1.4729056358337402\n",
            "------------------------\n",
            "Training done\n",
            "Model trained and stored at feedforwardnet.pth\n"
          ]
        }
      ]
    }
  ]
}